{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Modules and Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow scikit-learn matplotlib seaborn numpy pandas\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv1D, BatchNormalization, Activation, Dropout, GRU, Dense\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, AUC\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List all CSV files in the dataset folder\n",
    "dataset_folder = '../dataset'\n",
    "csv_files = [f for f in os.listdir(dataset_folder) if f.endswith('.csv')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all CSV files into a single pandas DataFrame\n",
    "dataset = []\n",
    "\n",
    "for episode_num, file in enumerate(csv_files):\n",
    "    episode = pd.read_csv(os.path.join(dataset_folder, file))\n",
    "    episode['episode'] = episode_num\n",
    "    dataset.append(episode)\n",
    "\n",
    "dataset = pd.concat(dataset)\n",
    "dataset = dataset.drop(columns=['timestamp'])\n",
    "\n",
    "# Get the target marix\n",
    "for i in range (1, 5):\n",
    "    dataset[f'motor_{i}'] = dataset['motor_failure'] == i\n",
    "dataset['motor_failure'] = dataset['motor_failure'] == 0\n",
    "\n",
    "# Squencing the dataset\n",
    "sequence_length = 10\n",
    "X_dataset = []\n",
    "y_dataset = []\n",
    "\n",
    "for episode in dataset['episode'].unique():\n",
    "    episode_dataset = dataset[dataset['episode'] == episode]\n",
    "    episode_dataset = episode_dataset.drop(columns=['episode'])\n",
    "    \n",
    "    for idx in range(episode_dataset.shape[0] - sequence_length):\n",
    "        X_episode = episode_dataset.drop(columns=['motor_failure', 'motor_1', 'motor_2', 'motor_3', 'motor_4']).iloc[idx : idx + sequence_length]\n",
    "        y_episode = episode_dataset[['motor_failure', 'motor_1', 'motor_2', 'motor_3', 'motor_4']].iloc[idx + sequence_length]\n",
    "        X_dataset.append(X_episode)\n",
    "        y_dataset.append(y_dataset)\n",
    "\n",
    "# Split the dataset into train and test\n",
    "X_dataset = np.array(X_dataset)\n",
    "y_dataset = np.array(y_dataset)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_dataset, y_dataset, test_size=0.3)\n",
    "\n",
    "# Output shapes for verification\n",
    "print(f'Training dataset shape: {X_train.shape}')\n",
    "print(f'Test dataset shape: {X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drone Failure Detection Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MotorFailureDetectionModel(tf.keras.Model):\n",
    "    def __init__(self, input_shape):\n",
    "        super(MotorFailureDetectionModel, self).__init__()\n",
    "        \n",
    "        # Step 1: CONV layer\n",
    "        self.conv1d = Conv1D(filters=196, kernel_size=15, strides=4, input_shape=input_shape)\n",
    "        self.batch_norm1 = BatchNormalization()\n",
    "        self.relu = Activation(\"relu\")\n",
    "        self.dropout1 = Dropout(rate=0.8)\n",
    "        \n",
    "        # Step 2: First GRU Layer\n",
    "        self.gru1 = GRU(units=128, return_sequences=True)\n",
    "        self.dropout2 = Dropout(rate=0.8)\n",
    "        self.batch_norm2 = BatchNormalization()\n",
    "        \n",
    "        # Step 3: Second GRU Layer\n",
    "        self.gru2 = GRU(units=128, return_sequences=True)\n",
    "        self.dropout3 = Dropout(rate=0.8)\n",
    "        self.batch_norm3 = BatchNormalization()\n",
    "        self.dropout4 = Dropout(rate=0.8)\n",
    "        \n",
    "        # Step 4: Dense output layer\n",
    "        self.output_layer = Dense(5, activation=\"sigmoid\")\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # Step 1: CONV layer\n",
    "        x = self.conv1d(inputs)\n",
    "        x = self.batch_norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        # Step 2: First GRU Layer\n",
    "        x = self.gru1(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.batch_norm2(x)\n",
    "        \n",
    "        # Step 3: Second GRU Layer\n",
    "        x = self.gru2(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.batch_norm3(x)\n",
    "        x = self.dropout4(x)\n",
    "        \n",
    "        # Step 4: Dense output layer\n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initialization\n",
    "input_shape = (sequence_length, X_dataset.shape[2])\n",
    "model = MotorFailureDetectionModel(input_shape=input_shape)\n",
    "model.build(input_shape=(None, sequence_length, X_dataset.shape[2]))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer\n",
    "loss_fn = BinaryCrossentropy()\n",
    "optimizer = Adam(learning_rate=0.001)\n",
    "metrics = [BinaryAccuracy(), AUC()]\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=optimizer, loss=loss_fn, metrics=metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the training parameters\n",
    "model_name = 'Failure-Detection-0.h5'\n",
    "epochs_num = 100\n",
    "batch_size = 4\n",
    "val_split = 0.2\n",
    "\n",
    "# Callbacks\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5),\n",
    "    ModelCheckpoint(filepath=f'../models/{model_name}', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Training\n",
    "history = model.fit(X_train, y_train, epochs=epochs_num, batch_size=batch_size, validation_split=val_split, callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate & Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the Model\n",
    "results = model.evaluate(X_test, y_test)\n",
    "\n",
    "print(f'Loss: {results[0]:.4f}')\n",
    "for metric_name, metric_value in zip(model.metrics_names[1:], results[1:]):\n",
    "    print(f'{metric_name}: {metric_value:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "model_name = 'Failure-Detection-0.h5'\n",
    "model.save(f'../models/{model_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model\n",
    "model_name = 'Failure-Detection-0.h5'\n",
    "model = load_model(f'../models/{model_name}')\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
